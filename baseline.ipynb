{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### split training and test sets of transcription ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def flatten(list_of_list):\n",
    "    return [item for sublist in list_of_list for item in sublist]\n",
    "\n",
    "path_to_training = Path(\"training\")\n",
    "path_to_test = Path(\"test\")\n",
    "\n",
    "def split_dataset(validate=False):\n",
    "    training_set = ['ES2002', 'ES2005', 'ES2006', 'ES2007', 'ES2008', 'ES2009', 'ES2010', 'ES2012', 'ES2013', 'ES2015', 'ES2016', 'IS1000', 'IS1001', 'IS1002', 'IS1003', 'IS1004', 'IS1005', 'IS1006', 'IS1007', 'TS3005', 'TS3008', 'TS3009', 'TS3010', 'TS3011', 'TS3012']\n",
    "    training_set = flatten([[m_id+s_id for s_id in 'abcd'] for m_id in training_set])\n",
    "    training_set.remove('IS1002a')\n",
    "    training_set.remove('IS1005d')\n",
    "    training_set.remove('TS3012c')\n",
    "\n",
    "    test_set = ['ES2003', 'ES2004', 'ES2011', 'ES2014', 'IS1008', 'IS1009', 'TS3003', 'TS3004', 'TS3006', 'TS3007']\n",
    "    test_set = flatten([[m_id+s_id for s_id in 'abcd'] for m_id in test_set])\n",
    "    \n",
    "    if validate:\n",
    "        # randomly select 10% of training set as validation set\n",
    "        import random\n",
    "        random.seed(6969)\n",
    "        validate_set = random.choices(training_set, k=int(len(training_set)*0.1))\n",
    "        training_set = list(set(training_set) - set(validate_set))\n",
    "        return training_set, validate_set, test_set\n",
    "\n",
    "    return training_set, test_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### functions to get features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import networkx as nx\n",
    "bert = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "\n",
    "def get_text_feature(dataset, path, show_progress_bar=True):\n",
    "    text_feature = []\n",
    "    for transcription_id in dataset:\n",
    "        with open(path / f\"{transcription_id}.json\", \"r\") as text_file:\n",
    "            transcription = json.load(text_file)\n",
    "        \n",
    "        for utterance in transcription:\n",
    "            text_feature.append(utterance[\"speaker\"] + \": \" + utterance[\"text\"])\n",
    "\n",
    "    text_feature = bert.encode(text_feature, show_progress_bar=show_progress_bar)\n",
    "    return text_feature\n",
    "\n",
    "\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "from itertools import chain\n",
    "\n",
    "def get_graph_feature(dataset, path, show_progress_bar=True):\n",
    "    graph_feature = []\n",
    "    \n",
    "    relation_mapping = {'nan': 0}  # 将np.nan映射为0\n",
    "    next_relation_id = 1\n",
    "    \n",
    "    for transcription_id in dataset:       \n",
    "        with open(path / f\"{transcription_id}.txt\", \"r\") as graph_file:\n",
    "            edges = []\n",
    "            relations = []\n",
    "            for line in graph_file:\n",
    "                parts = line.split()\n",
    "                source, relation, target = int(parts[0]), parts[1], int(parts[2])\n",
    "                \n",
    "                if relation not in relation_mapping:\n",
    "                    relation_mapping[relation] = next_relation_id\n",
    "                    next_relation_id += 1\n",
    "                \n",
    "                edges.append((source, target, {'relation': relation}))\n",
    "                relations.append(relation_mapping[relation])\n",
    "            \n",
    "        G = nx.DiGraph()\n",
    "        G.add_edges_from(edges)\n",
    "        \n",
    "        node_degrees = dict(G.degree())\n",
    "        \n",
    "        # 添加中心性度量，这里以度中心性为例\n",
    "        degree_centrality = nx.degree_centrality(G)\n",
    "        \n",
    "        # 处理叶子节点，将关系设置为nan\n",
    "        for node in G.nodes:\n",
    "            if G.out_degree(node) == 0:\n",
    "                relations.append(0)  # 将np.nan映射为0\n",
    "        \n",
    "        # 组合节点的度、关系、和中心性度量\n",
    "        combined_feature = list(zip(node_degrees.values(), relations, degree_centrality.values()))\n",
    "        graph_feature.extend(combined_feature)\n",
    "    \n",
    "    return graph_feature\n",
    "\n",
    "\n",
    "\n",
    "def get_label(dataset, label_file):\n",
    "    labels = []\n",
    "    with open(label_file, \"r\") as file:\n",
    "        all_labels = json.load(file)\n",
    "    for transcription_id in dataset:  \n",
    "        labels += all_labels[transcription_id]\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use only the text feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### load X and y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28ad56c6f60c491fa1cbef0f1878db87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/2058 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "training_set, validate_set, test_set = split_dataset(validate=True)\n",
    "\n",
    "X_training = get_text_feature(training_set, path_to_training)\n",
    "y_training = get_label(training_set, \"training_labels.json\")\n",
    "\n",
    "X_validate = get_text_feature(validate_set, path_to_training, show_progress_bar=False)\n",
    "y_validate = get_label(validate_set, \"training_labels.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### naive_baseline: \n",
    "##### all utterances are predicted important (label 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.8179729384733214\n"
     ]
    }
   ],
   "source": [
    "y_pred = [1] * len(y_validate)\n",
    "\n",
    "# print F1 score\n",
    "print(f1_score(y_validate, y_pred, average='binary'))\n",
    "# print accuracy\n",
    "print(sum([1 if y_pred[i] == y_validate[i] else 0 for i in range(len(y_validate))]) / len(y_validate))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### text_baseline(Decision Tree): \n",
    "##### utterances are embedded with SentenceTransformer, then train a Decision Tree classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.10745233968804159\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "clf = DecisionTreeClassifier(random_state=0)\n",
    "clf.fit(X_training, y_training)\n",
    "\n",
    "y_pred = clf.predict(X_validate).tolist()\n",
    "\n",
    "# print F1 score\n",
    "print(f1_score(y_validate, y_pred, average='binary'))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### text_baseline(Random Forest): \n",
    "##### utterances are embedded with SentenceTransformer, then train a Random Forest classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdbe4b61e98c474fbf04b1e78adc5dcd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/2058 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.25344036697247707\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=50, max_depth=5, criterion='gini', n_jobs=-1, random_state=0)\n",
    "clf.fit(X_training, y_training)\n",
    "\n",
    "y_pred = clf.predict(X_validate).tolist()\n",
    "\n",
    "# print F1 score\n",
    "print(f1_score(y_validate, y_pred, average='binary'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42511c49ac434ec28f52782df827858d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/2058 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running with n_estimators=100 and max_depth=10\n",
      "Running with n_estimators=100 and max_depth=20\n",
      "Running with n_estimators=200 and max_depth=10\n",
      "Running with n_estimators=200 and max_depth=20\n",
      "Running with n_estimators=300 and max_depth=10\n",
      "Running with n_estimators=300 and max_depth=20\n",
      "best_score:  0.26552706552706556\n",
      "best_parameter:  [100, 20]\n"
     ]
    }
   ],
   "source": [
    "# grid search\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "best_score = 0\n",
    "for n_estimators in [20, 35, 50, 75]:\n",
    "    for max_depth in [25, 30, 35]:\n",
    "\n",
    "        clf = RandomForestClassifier(n_estimators=n_estimators, max_depth=max_depth, criterion='gini', n_jobs=-1, random_state=0)\n",
    "        clf.fit(X_training, y_training)\n",
    "\n",
    "        y_pred = clf.predict(X_validate).tolist()\n",
    "        \n",
    "        score = f1_score(y_validate, y_pred, average='binary')\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_parameter = [n_estimators, max_depth]\n",
    "\n",
    "# print F1 score\n",
    "print(\"best_score: \", best_score)\n",
    "print(\"best_parameter: \", best_parameter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        # 取最后一个时间步的输出\n",
    "        lstm_out = lstm_out[:, -1, :]\n",
    "        output = self.fc(lstm_out)\n",
    "        output = self.sigmoid(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/25], Loss: 0.1977\n",
      "Epoch [2/25], Loss: 0.4408\n",
      "Epoch [3/25], Loss: 0.3709\n",
      "Epoch [4/25], Loss: 0.3527\n",
      "Epoch [5/25], Loss: 0.2323\n",
      "Epoch [6/25], Loss: 0.2660\n",
      "Epoch [7/25], Loss: 0.3239\n",
      "Epoch [8/25], Loss: 0.4742\n",
      "Epoch [9/25], Loss: 0.3395\n",
      "Epoch [10/25], Loss: 0.4157\n",
      "Epoch [11/25], Loss: 0.3573\n",
      "Epoch [12/25], Loss: 0.2794\n",
      "Epoch [13/25], Loss: 0.2279\n",
      "Epoch [14/25], Loss: 0.2168\n",
      "Epoch [15/25], Loss: 0.2851\n",
      "Epoch [16/25], Loss: 0.3030\n",
      "Epoch [17/25], Loss: 0.3077\n",
      "Epoch [18/25], Loss: 0.2993\n",
      "Epoch [19/25], Loss: 0.2509\n",
      "Epoch [20/25], Loss: 0.3114\n",
      "Epoch [21/25], Loss: 0.2200\n",
      "Epoch [22/25], Loss: 0.2309\n",
      "Epoch [23/25], Loss: 0.2212\n",
      "Epoch [24/25], Loss: 0.3107\n",
      "Epoch [25/25], Loss: 0.2190\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.optim as optim\n",
    "\n",
    "# 定义模型的超参数\n",
    "sequence_length = 10 # 假设你的每个对话的长度为 sequence_length\n",
    "input_size = X_training.shape[1]\n",
    "hidden_size = 64\n",
    "num_layers = 2\n",
    "output_size = 1  # 二元分类任务\n",
    "\n",
    "# 初始化模型\n",
    "model = LSTMClassifier(input_size, hidden_size, num_layers, output_size)\n",
    "\n",
    "# 定义损失函数和优化器\n",
    "criterion = nn.BCELoss()  # 二元交叉熵损失\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# 转换为 PyTorch 张量\n",
    "X_training_tensor = torch.tensor(X_training)\n",
    "y_training_tensor = torch.tensor(y_training, dtype=torch.int)\n",
    "X_validate_tensor = torch.tensor(X_validate)\n",
    "y_validate_tensor = torch.tensor(y_validate, dtype=torch.int)\n",
    "\n",
    "# 创建 TensorDataset\n",
    "train_dataset = TensorDataset(X_training_tensor, y_training_tensor)\n",
    "validate_dataset = TensorDataset(X_validate_tensor, y_validate_tensor)\n",
    "\n",
    "# 使用 DataLoader 加载数据\n",
    "batch_size = 64  # 你可以根据需要调整批量大小\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "validate_dataloader = DataLoader(validate_dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "# 模型训练\n",
    "num_epochs = 25\n",
    "for epoch in range(num_epochs):\n",
    "    for inputs, labels in train_dataloader:\n",
    "        inputs = inputs.unsqueeze(1)\n",
    "        labels = labels.unsqueeze(1)\n",
    "        # 梯度清零\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # 前向传播\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        # 计算损失\n",
    "        loss = criterion(outputs, labels.float())\n",
    "        \n",
    "        # 反向传播\n",
    "        loss.backward()\n",
    "        \n",
    "        # 参数更新\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-Score on validation set: 0.4972\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# 模型评估\n",
    "model.eval()\n",
    "all_predictions = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in validate_dataloader:\n",
    "        inputs = inputs.unsqueeze(1)\n",
    "        labels = labels.unsqueeze(1)\n",
    "        # 预测\n",
    "        outputs = model(inputs)\n",
    "        predictions = (outputs >= 0.5).int()\n",
    "        \n",
    "        # 保存预测值和标签\n",
    "        all_predictions.extend(predictions.numpy())\n",
    "        all_labels.extend(labels.numpy())\n",
    "\n",
    "# 计算f1-score\n",
    "f1 = f1_score(all_labels, all_predictions)\n",
    "print(f'F1-Score on validation set: {f1:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### generate submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c53fe6308781406c9754048efd8ca83a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/2270 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "training_set, test_set = split_dataset()\n",
    "\n",
    "X_training = get_text_feature(training_set, path_to_training)\n",
    "y_training = get_label(training_set, \"training_labels.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels = {}\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for transcription_id in test_set:\n",
    "        with open(path_to_test / f\"{transcription_id}.json\", \"r\") as file:\n",
    "            transcription = json.load(file)\n",
    "        \n",
    "        X_test = []\n",
    "        for utterance in transcription:\n",
    "            X_test.append(utterance[\"speaker\"] + \": \" + utterance[\"text\"])\n",
    "        \n",
    "        X_test = bert.encode(X_test)\n",
    "        X_test = torch.tensor(X_test).unsqueeze(1)\n",
    "\n",
    "        # y_test = clf.predict(X_test)\n",
    "        outputs = model(X_test)\n",
    "        y_test = (outputs >= 0.5).int()\n",
    "        y_test = y_test.squeeze(1)\n",
    "        test_labels[transcription_id] = y_test.tolist()\n",
    "\n",
    "with open(\"test_labels_text_baseline.json\", \"w\") as file:\n",
    "    json.dump(test_labels, file, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use the combination of text and graph feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### combine_baseline(Random Forest): \n",
    "##### utterances are embedded with SentenceTransformer, node degrees are used as graph feature, then train a Random Forest classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed5efed1e2e64a989d8c1bae44251161",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/2058 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3808854532677442\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import numpy as np\n",
    "\n",
    "training_set, validate_set, test_set = split_dataset(validate=True)\n",
    "\n",
    "train_text_feature = get_text_feature(training_set, path_to_training)\n",
    "train_graph_feature = get_graph_feature(training_set, path_to_training)\n",
    "X_training = [np.concatenate((text_feat, [graph_feat])) for text_feat, graph_feat in zip(train_text_feature, train_graph_feature)]\n",
    "y_training = get_label(training_set, \"training_labels.json\")\n",
    "\n",
    "clf = DecisionTreeClassifier(random_state=0)\n",
    "clf.fit(X_training, y_training)\n",
    "\n",
    "validate_text_feature = get_text_feature(validate_set, path_to_training, show_progress_bar=False)\n",
    "validate_graph_feature = get_graph_feature(validate_set, path_to_training)\n",
    "X_validate = [np.concatenate((text_feat, [graph_feat])) for text_feat, graph_feat in zip(validate_text_feature, validate_graph_feature)]\n",
    "y_validate = get_label(validate_set, \"training_labels.json\")\n",
    "\n",
    "y_pred = clf.predict(X_validate).tolist()\n",
    "\n",
    "# print F1 score\n",
    "print(f1_score(y_validate, y_pred, average='binary'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.5rc1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5rc1"
  },
  "vscode": {
   "interpreter": {
    "hash": "f6c3fe0f35ac205adeee3d33d4d27f45ea8735944f6bcc7cc4d72c9672a115a8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
