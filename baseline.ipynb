{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### split training and test sets of transcription ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def flatten(list_of_list):\n",
    "    return [item for sublist in list_of_list for item in sublist]\n",
    "\n",
    "path_to_training = Path(\"training\")\n",
    "path_to_test = Path(\"test\")\n",
    "\n",
    "def split_dataset(validate=False):\n",
    "    training_set = ['ES2002', 'ES2005', 'ES2006', 'ES2007', 'ES2008', 'ES2009', 'ES2010', 'ES2012', 'ES2013', 'ES2015', 'ES2016', 'IS1000', 'IS1001', 'IS1002', 'IS1003', 'IS1004', 'IS1005', 'IS1006', 'IS1007', 'TS3005', 'TS3008', 'TS3009', 'TS3010', 'TS3011', 'TS3012']\n",
    "    training_set = flatten([[m_id+s_id for s_id in 'abcd'] for m_id in training_set])\n",
    "    training_set.remove('IS1002a')\n",
    "    training_set.remove('IS1005d')\n",
    "    training_set.remove('TS3012c')\n",
    "\n",
    "    test_set = ['ES2003', 'ES2004', 'ES2011', 'ES2014', 'IS1008', 'IS1009', 'TS3003', 'TS3004', 'TS3006', 'TS3007']\n",
    "    test_set = flatten([[m_id+s_id for s_id in 'abcd'] for m_id in test_set])\n",
    "    \n",
    "    if validate:\n",
    "        # randomly select 10% of training set as validation set\n",
    "        import random\n",
    "        random.seed(6969)\n",
    "        validate_set = random.choices(training_set, k=int(len(training_set)*0.15))\n",
    "        training_set = list(set(training_set) - set(validate_set))\n",
    "        return training_set, validate_set, test_set\n",
    "\n",
    "    return training_set, test_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### functions to get features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import networkx as nx\n",
    "bert = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "\n",
    "def get_text_feature(dataset, path, show_progress_bar=True):\n",
    "    text_feature = []\n",
    "    for transcription_id in dataset:\n",
    "        with open(path / f\"{transcription_id}.json\", \"r\") as text_file:\n",
    "            transcription = json.load(text_file)\n",
    "        \n",
    "        for utterance in transcription:\n",
    "            text_feature.append(utterance[\"speaker\"] + \": \" + utterance[\"text\"])\n",
    "\n",
    "    text_feature = bert.encode(text_feature, show_progress_bar=show_progress_bar)\n",
    "    return text_feature\n",
    "\n",
    "\n",
    "def get_graph_feature(dataset, path, relation_mapping=None):\n",
    "    graph_feature = []\n",
    "    \n",
    "    if relation_mapping is None:\n",
    "        relation_mapping = {'nan': 0}  # 将np.nan映射为0\n",
    "        next_relation_id = 1\n",
    "    \n",
    "    for transcription_id in dataset:       \n",
    "        with open(path / f\"{transcription_id}.txt\", \"r\") as graph_file:\n",
    "            edges = []\n",
    "            relations = []\n",
    "            for line in graph_file:\n",
    "                parts = line.split()\n",
    "                source, relation, target = int(parts[0]), parts[1], int(parts[2])\n",
    "                \n",
    "                if relation not in relation_mapping:\n",
    "                    relation_mapping[relation] = next_relation_id\n",
    "                    next_relation_id += 1\n",
    "                \n",
    "                edges.append((source, target, {'relation': relation}))\n",
    "                relations.append(relation_mapping[relation])\n",
    "            \n",
    "        G = nx.DiGraph()\n",
    "        G.add_edges_from(edges)\n",
    "        \n",
    "        node_degrees = dict(G.degree())\n",
    "        \n",
    "        # 添加中心性度量，这里以度中心性为例\n",
    "        degree_centrality = nx.degree_centrality(G)\n",
    "        \n",
    "        # 处理叶子节点，将关系设置为nan\n",
    "        for node in G.nodes:\n",
    "            if G.out_degree(node) == 0:\n",
    "                relations.append(0)  # 将np.nan映射为0\n",
    "        \n",
    "        # 组合节点的度、关系、和中心性度量\n",
    "        combined_feature = list(zip(node_degrees.values(), relations, degree_centrality.values()))\n",
    "        graph_feature.extend(combined_feature)\n",
    "    \n",
    "    return graph_feature, relation_mapping\n",
    "\n",
    "\n",
    "\n",
    "def get_label(dataset, label_file):\n",
    "    labels = []\n",
    "    with open(label_file, \"r\") as file:\n",
    "        all_labels = json.load(file)\n",
    "    for transcription_id in dataset:  \n",
    "        labels += all_labels[transcription_id]\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use only the text feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### load X and y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28ad56c6f60c491fa1cbef0f1878db87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/2058 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "training_set, validate_set, test_set = split_dataset(validate=True)\n",
    "\n",
    "X_training = get_text_feature(training_set, path_to_training)\n",
    "y_training = get_label(training_set, \"training_labels.json\")\n",
    "\n",
    "X_validate = get_text_feature(validate_set, path_to_training, show_progress_bar=False)\n",
    "y_validate = get_label(validate_set, \"training_labels.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### naive_baseline: \n",
    "##### all utterances are predicted important (label 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.8179729384733214\n"
     ]
    }
   ],
   "source": [
    "y_pred = [1] * len(y_validate)\n",
    "\n",
    "# print F1 score\n",
    "print(f1_score(y_validate, y_pred, average='binary'))\n",
    "# print accuracy\n",
    "print(sum([1 if y_pred[i] == y_validate[i] else 0 for i in range(len(y_validate))]) / len(y_validate))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### text_baseline(Decision Tree): \n",
    "##### utterances are embedded with SentenceTransformer, then train a Decision Tree classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.10745233968804159\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "clf = DecisionTreeClassifier(random_state=0)\n",
    "clf.fit(X_training, y_training)\n",
    "\n",
    "y_pred = clf.predict(X_validate).tolist()\n",
    "\n",
    "# print F1 score\n",
    "print(f1_score(y_validate, y_pred, average='binary'))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### text_baseline(Random Forest): \n",
    "##### utterances are embedded with SentenceTransformer, then train a Random Forest classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdbe4b61e98c474fbf04b1e78adc5dcd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/2058 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.25344036697247707\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=50, max_depth=5, criterion='gini', n_jobs=-1, random_state=0)\n",
    "clf.fit(X_training, y_training)\n",
    "\n",
    "y_pred = clf.predict(X_validate).tolist()\n",
    "\n",
    "# print F1 score\n",
    "print(f1_score(y_validate, y_pred, average='binary'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42511c49ac434ec28f52782df827858d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/2058 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running with n_estimators=100 and max_depth=10\n",
      "Running with n_estimators=100 and max_depth=20\n",
      "Running with n_estimators=200 and max_depth=10\n",
      "Running with n_estimators=200 and max_depth=20\n",
      "Running with n_estimators=300 and max_depth=10\n",
      "Running with n_estimators=300 and max_depth=20\n",
      "best_score:  0.26552706552706556\n",
      "best_parameter:  [100, 20]\n"
     ]
    }
   ],
   "source": [
    "# grid search\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "best_score = 0\n",
    "for n_estimators in [20, 35, 50, 75]:\n",
    "    for max_depth in [25, 30, 35]:\n",
    "\n",
    "        clf = RandomForestClassifier(n_estimators=n_estimators, max_depth=max_depth, criterion='gini', n_jobs=-1, random_state=0)\n",
    "        clf.fit(X_training, y_training)\n",
    "\n",
    "        y_pred = clf.predict(X_validate).tolist()\n",
    "        \n",
    "        score = f1_score(y_validate, y_pred, average='binary')\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_parameter = [n_estimators, max_depth]\n",
    "\n",
    "# print F1 score\n",
    "print(\"best_score: \", best_score)\n",
    "print(\"best_parameter: \", best_parameter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### text_baseline(LSTM): \n",
    "##### utterances are embedded with SentenceTransformer, then train a Decision Tree classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        # 取最后一个时间步的输出\n",
    "        lstm_out = lstm_out[:, -1, :]\n",
    "        output = self.fc(lstm_out)\n",
    "        output = self.sigmoid(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/25], Loss: 0.1977\n",
      "Epoch [2/25], Loss: 0.4408\n",
      "Epoch [3/25], Loss: 0.3709\n",
      "Epoch [4/25], Loss: 0.3527\n",
      "Epoch [5/25], Loss: 0.2323\n",
      "Epoch [6/25], Loss: 0.2660\n",
      "Epoch [7/25], Loss: 0.3239\n",
      "Epoch [8/25], Loss: 0.4742\n",
      "Epoch [9/25], Loss: 0.3395\n",
      "Epoch [10/25], Loss: 0.4157\n",
      "Epoch [11/25], Loss: 0.3573\n",
      "Epoch [12/25], Loss: 0.2794\n",
      "Epoch [13/25], Loss: 0.2279\n",
      "Epoch [14/25], Loss: 0.2168\n",
      "Epoch [15/25], Loss: 0.2851\n",
      "Epoch [16/25], Loss: 0.3030\n",
      "Epoch [17/25], Loss: 0.3077\n",
      "Epoch [18/25], Loss: 0.2993\n",
      "Epoch [19/25], Loss: 0.2509\n",
      "Epoch [20/25], Loss: 0.3114\n",
      "Epoch [21/25], Loss: 0.2200\n",
      "Epoch [22/25], Loss: 0.2309\n",
      "Epoch [23/25], Loss: 0.2212\n",
      "Epoch [24/25], Loss: 0.3107\n",
      "Epoch [25/25], Loss: 0.2190\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.optim as optim\n",
    "\n",
    "# 定义模型的超参数\n",
    "sequence_length = 10 # 假设你的每个对话的长度为 sequence_length\n",
    "input_size = X_training.shape[1]\n",
    "hidden_size = 64\n",
    "num_layers = 2\n",
    "output_size = 1  # 二元分类任务\n",
    "\n",
    "# 初始化模型\n",
    "model = LSTMClassifier(input_size, hidden_size, num_layers, output_size)\n",
    "\n",
    "# 定义损失函数和优化器\n",
    "criterion = nn.BCELoss()  # 二元交叉熵损失\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# 转换为 PyTorch 张量\n",
    "X_training_tensor = torch.tensor(X_training)\n",
    "y_training_tensor = torch.tensor(y_training, dtype=torch.int)\n",
    "X_validate_tensor = torch.tensor(X_validate)\n",
    "y_validate_tensor = torch.tensor(y_validate, dtype=torch.int)\n",
    "\n",
    "# 创建 TensorDataset\n",
    "train_dataset = TensorDataset(X_training_tensor, y_training_tensor)\n",
    "validate_dataset = TensorDataset(X_validate_tensor, y_validate_tensor)\n",
    "\n",
    "# 使用 DataLoader 加载数据\n",
    "batch_size = 64  # 你可以根据需要调整批量大小\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "validate_dataloader = DataLoader(validate_dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "# 模型训练\n",
    "num_epochs = 25\n",
    "for epoch in range(num_epochs):\n",
    "    for inputs, labels in train_dataloader:\n",
    "        inputs = inputs.unsqueeze(1)\n",
    "        labels = labels.unsqueeze(1)\n",
    "        # 梯度清零\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # 前向传播\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        # 计算损失\n",
    "        loss = criterion(outputs, labels.float())\n",
    "        \n",
    "        # 反向传播\n",
    "        loss.backward()\n",
    "        \n",
    "        # 参数更新\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-Score on validation set: 0.3367\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# 模型评估\n",
    "model.eval()\n",
    "all_predictions = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in validate_dataloader:\n",
    "        inputs = inputs.unsqueeze(1)\n",
    "        labels = labels.unsqueeze(1)\n",
    "        # 预测\n",
    "        outputs = model(inputs)\n",
    "        predictions = (outputs >= 0.5).int()\n",
    "        \n",
    "        # 保存预测值和标签\n",
    "        all_predictions.extend(predictions.numpy())\n",
    "        all_labels.extend(labels.numpy())\n",
    "\n",
    "# 计算f1-score\n",
    "f1 = f1_score(all_labels, all_predictions)\n",
    "print(f'F1-Score on validation set: {f1:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### generate submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c53fe6308781406c9754048efd8ca83a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/2270 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "training_set, test_set = split_dataset()\n",
    "\n",
    "X_training = get_text_feature(training_set, path_to_training)\n",
    "y_training = get_label(training_set, \"training_labels.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels = {}\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for transcription_id in test_set:\n",
    "        with open(path_to_test / f\"{transcription_id}.json\", \"r\") as file:\n",
    "            transcription = json.load(file)\n",
    "        \n",
    "        X_test = []\n",
    "        for utterance in transcription:\n",
    "            X_test.append(utterance[\"speaker\"] + \": \" + utterance[\"text\"])\n",
    "        \n",
    "        X_test = bert.encode(X_test)\n",
    "        X_test = torch.tensor(X_test).unsqueeze(1)\n",
    "\n",
    "        # y_test = clf.predict(X_test)\n",
    "        outputs = model(X_test)\n",
    "        y_test = (outputs >= 0.5).int()\n",
    "        y_test = y_test.squeeze(1)\n",
    "        test_labels[transcription_id] = y_test.tolist()\n",
    "\n",
    "with open(\"test_labels_text_baseline.json\", \"w\") as file:\n",
    "    json.dump(test_labels, file, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use the combination of text and graph feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### combine_baseline(Random Forest): \n",
    "##### utterances are embedded with SentenceTransformer, node degrees are used as graph feature, then train a Random Forest classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b11ee399e50407582072345c0acc886",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/2058 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "all the input arrays must have same number of dimensions, but the array at index 0 has 1 dimension(s) and the array at index 1 has 2 dimension(s)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32me:\\Polytechnique\\M1\\INF554\\INF554_Project\\baseline.ipynb 单元格 24\u001b[0m line \u001b[0;36m9\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/Polytechnique/M1/INF554/INF554_Project/baseline.ipynb#X32sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m train_text_feature \u001b[39m=\u001b[39m get_text_feature(training_set, path_to_training)\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/Polytechnique/M1/INF554/INF554_Project/baseline.ipynb#X32sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m train_graph_feature, relation_mapping \u001b[39m=\u001b[39m get_graph_feature(training_set, path_to_training)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/e%3A/Polytechnique/M1/INF554/INF554_Project/baseline.ipynb#X32sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m X_training \u001b[39m=\u001b[39m [np\u001b[39m.\u001b[39mconcatenate((text_feat, [graph_feat])) \u001b[39mfor\u001b[39;00m text_feat, graph_feat \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(train_text_feature, train_graph_feature)]\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/Polytechnique/M1/INF554/INF554_Project/baseline.ipynb#X32sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m y_training \u001b[39m=\u001b[39m get_label(training_set, \u001b[39m\"\u001b[39m\u001b[39mtraining_labels.json\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/Polytechnique/M1/INF554/INF554_Project/baseline.ipynb#X32sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m clf \u001b[39m=\u001b[39m DecisionTreeClassifier(random_state\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n",
      "\u001b[1;32me:\\Polytechnique\\M1\\INF554\\INF554_Project\\baseline.ipynb 单元格 24\u001b[0m line \u001b[0;36m9\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/Polytechnique/M1/INF554/INF554_Project/baseline.ipynb#X32sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m train_text_feature \u001b[39m=\u001b[39m get_text_feature(training_set, path_to_training)\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/Polytechnique/M1/INF554/INF554_Project/baseline.ipynb#X32sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m train_graph_feature, relation_mapping \u001b[39m=\u001b[39m get_graph_feature(training_set, path_to_training)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/e%3A/Polytechnique/M1/INF554/INF554_Project/baseline.ipynb#X32sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m X_training \u001b[39m=\u001b[39m [np\u001b[39m.\u001b[39;49mconcatenate((text_feat, [graph_feat])) \u001b[39mfor\u001b[39;00m text_feat, graph_feat \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(train_text_feature, train_graph_feature)]\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/Polytechnique/M1/INF554/INF554_Project/baseline.ipynb#X32sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m y_training \u001b[39m=\u001b[39m get_label(training_set, \u001b[39m\"\u001b[39m\u001b[39mtraining_labels.json\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/Polytechnique/M1/INF554/INF554_Project/baseline.ipynb#X32sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m clf \u001b[39m=\u001b[39m DecisionTreeClassifier(random_state\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n",
      "File \u001b[1;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mconcatenate\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: all the input arrays must have same number of dimensions, but the array at index 0 has 1 dimension(s) and the array at index 1 has 2 dimension(s)"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import numpy as np\n",
    "\n",
    "training_set, validate_set, test_set = split_dataset(validate=True)\n",
    "\n",
    "train_text_feature = get_text_feature(training_set, path_to_training)\n",
    "train_graph_feature, relation_mapping = get_graph_feature(training_set, path_to_training)\n",
    "X_training = [np.concatenate((text_feat, [graph_feat])) for text_feat, graph_feat in zip(train_text_feature, train_graph_feature)]\n",
    "y_training = get_label(training_set, \"training_labels.json\")\n",
    "\n",
    "clf = DecisionTreeClassifier(random_state=0)\n",
    "clf.fit(X_training, y_training)\n",
    "\n",
    "validate_text_feature = get_text_feature(validate_set, path_to_training, show_progress_bar=False)\n",
    "validate_graph_feature, _ = get_graph_feature(validate_set, path_to_training, relation_mapping)\n",
    "X_validate = [np.concatenate((text_feat, [graph_feat])) for text_feat, graph_feat in zip(validate_text_feature, validate_graph_feature)]\n",
    "y_validate = get_label(validate_set, \"training_labels.json\")\n",
    "\n",
    "y_pred = clf.predict(X_validate).tolist()\n",
    "\n",
    "# print F1 score\n",
    "print(f1_score(y_validate, y_pred, average='binary'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Two model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "373ce5f6bc9343feb2227b100e1a5d15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/2058 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "training_set, validate_set, test_set = split_dataset(validate=True)\n",
    "\n",
    "text_feature_training = get_text_feature(training_set, path_to_training)\n",
    "graph_feature_training, relation_mapping = get_graph_feature(training_set, path_to_training)\n",
    "y_training = get_label(training_set, \"training_labels.json\")\n",
    "\n",
    "text_feature_validate = get_text_feature(validate_set, path_to_training, show_progress_bar=False)\n",
    "graph_feature_validate, _ = get_graph_feature(validate_set, path_to_training, relation_mapping)\n",
    "y_validate = get_label(validate_set, \"training_labels.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use LSTM for text and Decision Tree for graph "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        # 取最后一个时间步的输出\n",
    "        lstm_out = lstm_out[:, -1, :]\n",
    "        output = self.fc(lstm_out)\n",
    "        output = self.sigmoid(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/25], Loss: 0.2749\n",
      "Epoch [2/25], Loss: 0.2464\n",
      "Epoch [3/25], Loss: 0.4448\n",
      "Epoch [4/25], Loss: 0.4203\n",
      "Epoch [5/25], Loss: 0.2241\n",
      "Epoch [6/25], Loss: 0.4276\n",
      "Epoch [7/25], Loss: 0.3122\n",
      "Epoch [8/25], Loss: 0.4052\n",
      "Epoch [9/25], Loss: 0.3080\n",
      "Epoch [10/25], Loss: 0.3564\n",
      "Epoch [11/25], Loss: 0.3116\n",
      "Epoch [12/25], Loss: 0.3891\n",
      "Epoch [13/25], Loss: 0.3477\n",
      "Epoch [14/25], Loss: 0.1943\n",
      "Epoch [15/25], Loss: 0.3347\n",
      "Epoch [16/25], Loss: 0.3200\n",
      "Epoch [17/25], Loss: 0.3741\n",
      "Epoch [18/25], Loss: 0.2742\n",
      "Epoch [19/25], Loss: 0.2071\n",
      "Epoch [20/25], Loss: 0.3553\n",
      "Epoch [21/25], Loss: 0.3274\n",
      "Epoch [22/25], Loss: 0.1723\n",
      "Epoch [23/25], Loss: 0.2498\n",
      "Epoch [24/25], Loss: 0.4203\n",
      "Epoch [25/25], Loss: 0.3000\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.optim as optim\n",
    "\n",
    "# 定义模型的超参数\n",
    "sequence_length = 10 # 假设你的每个对话的长度为 sequence_length\n",
    "input_size = text_feature_training.shape[1]\n",
    "hidden_size = 64\n",
    "num_layers = 2\n",
    "output_size = 1  # 二元分类任务\n",
    "\n",
    "# 初始化模型\n",
    "model = LSTMClassifier(input_size, hidden_size, num_layers, output_size)\n",
    "\n",
    "# 定义损失函数和优化器\n",
    "criterion = nn.BCELoss()  # 二元交叉熵损失\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# 转换为 PyTorch 张量\n",
    "X_training_tensor = torch.tensor(text_feature_training)\n",
    "y_training_tensor = torch.tensor(y_training, dtype=torch.int)\n",
    "X_validate_tensor = torch.tensor(text_feature_validate)\n",
    "y_validate_tensor = torch.tensor(y_validate, dtype=torch.int)\n",
    "\n",
    "# 创建 TensorDataset\n",
    "train_dataset = TensorDataset(X_training_tensor, y_training_tensor)\n",
    "validate_dataset = TensorDataset(X_validate_tensor, y_validate_tensor)\n",
    "\n",
    "# 使用 DataLoader 加载数据\n",
    "batch_size = 64  # 你可以根据需要调整批量大小\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "validate_dataloader = DataLoader(validate_dataset, batch_size=1)\n",
    "\n",
    "# 模型训练\n",
    "num_epochs = 25\n",
    "for epoch in range(num_epochs):\n",
    "    for inputs, labels in train_dataloader:\n",
    "        inputs = inputs.unsqueeze(1)\n",
    "        labels = labels.unsqueeze(1)\n",
    "        # 梯度清零\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # 前向传播\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        # 计算损失\n",
    "        loss = criterion(outputs, labels.float())\n",
    "        \n",
    "        # 反向传播\n",
    "        loss.backward()\n",
    "        \n",
    "        # 参数更新\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-Score on validation set: 0.5541\n"
     ]
    }
   ],
   "source": [
    "# 模型评估\n",
    "model.eval()\n",
    "y_text_pred = []\n",
    "text_prob = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in validate_dataloader:\n",
    "        inputs = inputs.unsqueeze(1)\n",
    "        labels = labels.unsqueeze(1)\n",
    "        # 预测\n",
    "        outputs = model(inputs)\n",
    "        predictions = (outputs >= 0.5).int()\n",
    "        \n",
    "        # 保存预测值和标签\n",
    "        text_prob.extend(outputs.numpy())\n",
    "        y_text_pred.extend(predictions.numpy().tolist())\n",
    "\n",
    "# 计算f1-score\n",
    "y_text_pred = np.array(y_text_pred).flatten().tolist()\n",
    "f1 = f1_score(y_validate, y_text_pred)\n",
    "print(f'F1-Score on validation set: {f1:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.08633093525179857\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "clf = DecisionTreeClassifier(min_samples_leaf=1, min_samples_split=6, random_state=0)\n",
    "clf.fit(graph_feature_training, y_training)\n",
    "\n",
    "y_graph_pred = clf.predict(graph_feature_validate).tolist()\n",
    "\n",
    "# print F1 score\n",
    "print(f1_score(y_validate, y_graph_pred, average='binary'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_score:  0.09194029850746267\n",
      "best_parameter:  [1, 6]\n"
     ]
    }
   ],
   "source": [
    "# grid search\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "best_score = 0\n",
    "for min_samples_leaf in [1,3,5,7]:\n",
    "    for min_samples_split in [2,6,10]:\n",
    "\n",
    "        clf = DecisionTreeClassifier(min_samples_leaf=min_samples_leaf, min_samples_split=min_samples_split, random_state=0)\n",
    "        clf.fit(graph_feature_training, y_training)\n",
    "\n",
    "        y_pred = clf.predict(graph_feature_validate).tolist()\n",
    "        \n",
    "        score = f1_score(y_validate, y_pred, average='binary')\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_parameter = [min_samples_leaf, min_samples_split]\n",
    "\n",
    "# print F1 score\n",
    "print(\"best_score: \", best_score)\n",
    "print(\"best_parameter: \", best_parameter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.11602527283170591\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "clf = XGBClassifier(n_estimators=100, max_depth=25, objective='binary:logistic', n_jobs=-1, random_state=0)\n",
    "clf.fit(graph_feature_training, y_training)\n",
    "\n",
    "y_graph_pred = clf.predict(graph_feature_validate).tolist()\n",
    "graph_prob = clf.predict_proba(graph_feature_validate).tolist()\n",
    "\n",
    "# print F1 score\n",
    "print(f1_score(y_validate, y_graph_pred, average='binary'))\n",
    "graph_prob = [prob[1] for prob in graph_prob]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_score:  0.11602527283170591\n",
      "best_parameter:  [100, 25]\n"
     ]
    }
   ],
   "source": [
    "# grid search\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "best_score = 0\n",
    "for n_estimators in [25, 50, 75, 100, 125]:\n",
    "    for max_depth in [5, 15, 25, 35, 45]:\n",
    "\n",
    "        clf = XGBClassifier(n_estimators=n_estimators, max_depth=max_depth, random_state=0)\n",
    "        clf.fit(graph_feature_training, y_training)\n",
    "\n",
    "        y_pred = clf.predict(graph_feature_validate).tolist()\n",
    "        \n",
    "        score = f1_score(y_validate, y_pred, average='binary')\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_parameter = [n_estimators, max_depth]\n",
    "\n",
    "# print F1 score\n",
    "print(\"best_score: \", best_score)\n",
    "print(\"best_parameter: \", best_parameter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-Score on validation set: 0.5765\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# 将 LSTM 和 XGBoost 的预测结果水平拼接\n",
    "combined_predictions = np.column_stack((y_text_pred, y_graph_pred))\n",
    "\n",
    "# 划分数据集为训练和测试集\n",
    "X_train, X_test, y_train, y_test = train_test_split(combined_predictions, y_validate, test_size=0.2, random_state=6969)\n",
    "\n",
    "# 初始化逻辑回归模型（或其他模型）\n",
    "logistic_model = LogisticRegression()\n",
    "\n",
    "# 训练逻辑回归模型\n",
    "logistic_model.fit(X_train, y_train)\n",
    "\n",
    "# 进行预测\n",
    "final_predictions = logistic_model.predict(X_test)\n",
    "\n",
    "# 计算 F1-Score\n",
    "f1 = f1_score(y_test, final_predictions)\n",
    "print(f'F1-Score on validation set: {f1:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将 LSTM 和 XGBoost 的预测结果水平拼接\n",
    "combined_predictions = np.column_stack((y_text_pred, y_graph_pred))\n",
    "# 初始化逻辑回归模型（或其他模型）\n",
    "logistic_model = LogisticRegression()\n",
    "# 训练逻辑回归模型\n",
    "logistic_model.fit(combined_predictions, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set, validate_set, test_set = split_dataset(validate=True)\n",
    "y_training = get_label(training_set, \"training_labels.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use GNN for combined feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "from sentence_transformers import SentenceTransformer\n",
    "bert = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "def get_combine_feature(dataset, path, label_file, relation_mapping=None, for_test=False):\n",
    "    graph_dataset = []\n",
    "    if not for_test:\n",
    "        with open(label_file, \"r\") as file:\n",
    "            all_labels = json.load(file)\n",
    "        \n",
    "    # 读取图数据\n",
    "    if relation_mapping is None:\n",
    "        relation_mapping = {'nan': 0}  # 将np.nan映射为0\n",
    "        next_relation_id = 1\n",
    "    for transcription_id in dataset:       \n",
    "        with open(path / f\"{transcription_id}.txt\", \"r\") as graph_file:\n",
    "            lines = graph_file.readlines()\n",
    "\n",
    "        edges_list = []\n",
    "        for line in lines:\n",
    "            parts = line.split()\n",
    "            if len(parts) == 3:\n",
    "                src, relation, dest = int(parts[0]), parts[1], int(parts[2])\n",
    "                if relation not in relation_mapping:\n",
    "                    relation_mapping[relation] = next_relation_id\n",
    "                    next_relation_id += 1\n",
    "                edges_list.append((src, dest, relation_mapping[relation]))\n",
    "\n",
    "        # 读取节点属性\n",
    "        text_feature = []\n",
    "        with open(path / f\"{transcription_id}.json\", \"r\") as text_file:\n",
    "            transcription = json.load(text_file)\n",
    "        for utterance in transcription:\n",
    "            text_feature.append(utterance[\"speaker\"] + \": \" + utterance[\"text\"])\n",
    "        node_features = bert.encode(text_feature)\n",
    "        # node_features = torch.ones((len(all_labels[transcription_id]), 1), dtype=torch.float)\n",
    "        node_attributes = torch.tensor(node_features)\n",
    "\n",
    "        # 创建 PyTorch Geometric Data 对象\n",
    "        x = torch.tensor(node_attributes, dtype=torch.float)\n",
    "\n",
    "        # 将边列表转换为 PyTorch Geometric edge_index\n",
    "        src_nodes, dest_nodes, relations = zip(*edges_list)\n",
    "        edge_index = torch.tensor([src_nodes, dest_nodes], dtype=torch.long)\n",
    "\n",
    "        # 将边属性转换为 PyTorch Geometric edge_attr\n",
    "        edge_attr = torch.tensor(relations, dtype=torch.float).view(1, -1)\n",
    "\n",
    "        # 创建 PyTorch Geometric Data 对象\n",
    "        if for_test:\n",
    "            data = Data(x=x, edge_index=edge_index, edge_attr=edge_attr)\n",
    "        else:\n",
    "            data = Data(x=x, edge_index=edge_index, edge_attr=edge_attr, y=torch.tensor(all_labels[transcription_id]))\n",
    "        \n",
    "        graph_dataset.append(data)\n",
    "\n",
    "    return graph_dataset, relation_mapping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\17269\\AppData\\Local\\Temp\\ipykernel_12192\\4178636857.py:42: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x = torch.tensor(node_attributes, dtype=torch.float)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(x=[1095, 384], edge_index=[2, 1094], edge_attr=[1, 1094], y=[1095])\n",
      "Data(x=[1017, 384], edge_index=[2, 1016], edge_attr=[1, 1016], y=[1017])\n"
     ]
    }
   ],
   "source": [
    "# 处理多个图的数据\n",
    "training_set, validate_set, test_set = split_dataset(validate=True)\n",
    "\n",
    "train_dataset, relation_mapping = get_combine_feature(training_set, path_to_training, \"training_labels.json\")\n",
    "validate_dataset, _ = get_combine_feature(validate_set, path_to_training, \"training_labels.json\", relation_mapping)\n",
    "\n",
    "# 创建 DataLoader 用于批处理\n",
    "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "validate_loader = DataLoader(validate_dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "# 输出第一个图的信息\n",
    "print(train_loader.dataset[0])\n",
    "print(validate_loader.dataset[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/90, Loss: 0.35999660566449165, F1-Score: 0.0\n",
      "Epoch 18/90, Loss: 0.33149401541976703, F1-Score: 0.0027855153203342614\n",
      "Epoch 27/90, Loss: 0.2943440569298608, F1-Score: 0.5412234042553192\n",
      "Epoch 36/90, Loss: 0.26280111721938565, F1-Score: 0.5421605904719301\n",
      "Epoch 45/90, Loss: 0.21351825006838357, F1-Score: 0.5278396436525613\n",
      "Epoch 54/90, Loss: 0.18835431261963786, F1-Score: 0.5341414141414141\n",
      "Epoch 63/90, Loss: 0.16288100708542125, F1-Score: 0.5144380275433141\n",
      "Epoch 72/90, Loss: 0.1397091999672176, F1-Score: 0.4738206324520477\n",
      "Epoch 81/90, Loss: 0.1261899534867899, F1-Score: 0.48310328415040454\n",
      "Epoch 90/90, Loss: 0.11468902291796569, F1-Score: 0.4638507385332988\n",
      "best val f1:  0.5549695740365111\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch_geometric.nn import GCNConv, GATConv, SAGEConv\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def eval(model):\n",
    "    # 评估模型\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in validate_loader:\n",
    "            output = model(batch)\n",
    "            predictions = torch.argmax(output, dim=1)\n",
    "            all_predictions.extend(predictions.cpu().numpy())\n",
    "            all_labels.extend(batch.y.cpu().numpy())\n",
    "\n",
    "    # 计算F1-Score\n",
    "    f1 = f1_score(all_labels, all_predictions, average='binary')\n",
    "    return f1\n",
    "\n",
    "# 定义 GNN 模型\n",
    "class GCNModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(GCNModel, self).__init__()\n",
    "        self.conv1 = GCNConv(input_dim, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = torch.relu(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x\n",
    "\n",
    "\n",
    "class GATModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_heads=1):\n",
    "        super(GATModel, self).__init__()\n",
    "        self.convs = nn.ModuleList([\n",
    "            GATConv(input_dim, hidden_dim, heads=num_heads),\n",
    "            GATConv(hidden_dim * num_heads, hidden_dim, heads=num_heads)\n",
    "        ])\n",
    "        # Fully connected layer for classification\n",
    "        self.fc = nn.Linear(hidden_dim * num_heads, output_dim)\n",
    "        \n",
    "        # Dropout for regularization\n",
    "        self.dropout = nn.Dropout(p=0.3)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        \n",
    "        for conv in self.convs:\n",
    "            x = F.relu(conv(x, edge_index))\n",
    "            x = self.dropout(x)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class GraphSAGEModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, dropout=0.3):\n",
    "        super(GraphSAGEModel, self).__init__()\n",
    "    #     self.convs = nn.ModuleList([\n",
    "    #         SAGEConv(input_dim, hidden_dim),\n",
    "    #         SAGEConv(hidden_dim, output_dim)\n",
    "    #     ])\n",
    "    #     self.dropout = nn.Dropout(p=0.3)\n",
    "\n",
    "    # def forward(self, data):\n",
    "    #     x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "    #     for conv in self.convs:\n",
    "    #         x = conv(x, edge_index)\n",
    "    #         x = F.relu(x)\n",
    "    #         x = self.dropout(x)\n",
    "    #     return x\n",
    "        # List to hold multiple GraphSAGE layers\n",
    "        self.convs = nn.ModuleList([\n",
    "            SAGEConv(input_dim, hidden_dim*4),\n",
    "            SAGEConv(hidden_dim*4, hidden_dim*2),\n",
    "            SAGEConv(hidden_dim*2, hidden_dim),\n",
    "            SAGEConv(hidden_dim, hidden_dim)\n",
    "        ])\n",
    "\n",
    "        # Attention mechanism\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, 1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "        # Output layer\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "        # Dropout for regularization\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "\n",
    "        # Apply multiple GraphSAGE layers\n",
    "        for conv in self.convs:\n",
    "            x = F.relu(conv(x, edge_index))\n",
    "            x = self.dropout(x)\n",
    "\n",
    "        # Attention mechanism\n",
    "        attention_weights = F.softmax(self.attention(x), dim=0)\n",
    "        x = x * attention_weights\n",
    "\n",
    "        # Fully connected layer for final prediction\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "# 初始化模型、损失函数和优化器\n",
    "# model = GCNModel(input_dim=384, hidden_dim=64, output_dim=2)\n",
    "# model = GATModel(input_dim=384, hidden_dim=128, output_dim=2, num_heads=2)\n",
    "model = GraphSAGEModel(input_dim=384, hidden_dim=32, output_dim=2, dropout=0.2)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.005)\n",
    "\n",
    "# 训练模型\n",
    "best_val_f1 = 0.52\n",
    "num_epochs = 90\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(batch)\n",
    "        y = batch.y\n",
    "        # y = y.float().unsqueeze(1)\n",
    "        loss = criterion(output, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    if (epoch+1) % 3 == 0:\n",
    "        f1 = eval(model)\n",
    "        if f1 > best_val_f1:\n",
    "            best_val_f1 = f1\n",
    "            torch.save(model.state_dict(), \"best_model.pth\")\n",
    "    if (epoch+1) % 9 == 0:\n",
    "        average_loss = total_loss / len(train_loader)\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {average_loss}, F1-Score: {f1}\")\n",
    "        \n",
    "print(\"best val f1: \", best_val_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### generate result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\17269\\AppData\\Local\\Temp\\ipykernel_35368\\4178636857.py:42: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x = torch.tensor(node_attributes, dtype=torch.float)\n"
     ]
    }
   ],
   "source": [
    "# 处理多个图的数据\n",
    "training_set, test_set = split_dataset()\n",
    "\n",
    "train_dataset, relation_mapping = get_combine_feature(training_set, path_to_training, \"training_labels.json\")\n",
    "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(\"best_model.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\17269\\AppData\\Local\\Temp\\ipykernel_12192\\846226499.py:26: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x = torch.tensor(node_attributes, dtype=torch.float)\n"
     ]
    }
   ],
   "source": [
    "test_labels = {}\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for transcription_id in test_set:       \n",
    "        with open(path_to_test / f\"{transcription_id}.txt\", \"r\") as graph_file:\n",
    "            lines = graph_file.readlines()\n",
    "\n",
    "        edges_list = []\n",
    "        for line in lines:\n",
    "            parts = line.split()\n",
    "            if len(parts) == 3:\n",
    "                src, relation, dest = int(parts[0]), parts[1], int(parts[2])\n",
    "                edges_list.append((src, dest, relation_mapping[relation]))\n",
    "\n",
    "        # 读取节点属性\n",
    "        text_feature = []\n",
    "        with open(path_to_test / f\"{transcription_id}.json\", \"r\") as text_file:\n",
    "            transcription = json.load(text_file)\n",
    "        for utterance in transcription:\n",
    "            text_feature.append(utterance[\"speaker\"] + \": \" + utterance[\"text\"])\n",
    "        node_features = bert.encode(text_feature)\n",
    "        # node_features = torch.ones((len(all_labels[transcription_id]), 1), dtype=torch.float)\n",
    "        node_attributes = torch.tensor(node_features)\n",
    "\n",
    "        # 创建 PyTorch Geometric Data 对象\n",
    "        x = torch.tensor(node_attributes, dtype=torch.float)\n",
    "\n",
    "        # 将边列表转换为 PyTorch Geometric edge_index\n",
    "        src_nodes, dest_nodes, relations = zip(*edges_list)\n",
    "        edge_index = torch.tensor([src_nodes, dest_nodes], dtype=torch.long)\n",
    "\n",
    "        # 将边属性转换为 PyTorch Geometric edge_attr\n",
    "        edge_attr = torch.tensor(relations, dtype=torch.float).view(1, -1)\n",
    "\n",
    "        # 创建 PyTorch Geometric Data 对象\n",
    "        data = Data(x=x, edge_index=edge_index, edge_attr=edge_attr)\n",
    "        outputs = model(data)\n",
    "        predictions = torch.argmax(outputs, dim=1).int()\n",
    "        test_labels[transcription_id] = predictions.numpy().tolist()\n",
    "\n",
    "with open(\"test_labels_text_baseline.json\", \"w\") as file:\n",
    "    json.dump(test_labels, file, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.5rc1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "vscode": {
   "interpreter": {
    "hash": "f6c3fe0f35ac205adeee3d33d4d27f45ea8735944f6bcc7cc4d72c9672a115a8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
